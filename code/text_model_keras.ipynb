{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, hamming_loss, accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = pd.read_pickle('train_images_valid.pickle')\n",
    "img_test = pd.read_pickle('test_images_valid.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_train = pd.read_pickle('all_books_train.pickle')\n",
    "books_test = pd.read_pickle('all_books_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36389, 14)\n",
      "(12131, 14)\n"
     ]
    }
   ],
   "source": [
    "print(books_train.shape)\n",
    "print(books_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_train = books_train[books_train.index.isin(img_train.index)]\n",
    "books_test = books_test[books_test.index.isin(img_test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36298, 14)\n",
      "(12096, 14)\n"
     ]
    }
   ],
   "source": [
    "print(books_train.shape)\n",
    "print(books_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_genres(genre):\n",
    "    genres = genre.split('|')\n",
    "    return list(set(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'books' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f2c1a0ec6a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genres_cut'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'books' is not defined"
     ]
    }
   ],
   "source": [
    "len(set([item for sublist in books['genres_cut'] for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    leave_tokens = [token[0] for token in pos if token[1].startswith('VB') or token[1].startswith('JJ') or \n",
    "                   token[1] =='NN' or token[1] =='NNS']\n",
    "    return ' '.join(leave_tokens)\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = text.split(' ')\n",
    "    lemm_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemm_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    clean = clean_text(text)\n",
    "    token = tokenize(clean)\n",
    "    lemm = lemmatize(token)\n",
    "    return lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = books_train[['book_desc', 'genres_cut']]\n",
    "test = books_test[['book_desc', 'genres_cut']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_train['desc_proc'] = books_train['book_desc'].apply(preprocess)\n",
    "books_test['desc_proc'] = books_test['book_desc'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(books_train['genres_cut'])\n",
    "y_train = multilabel_binarizer.transform(books_train['genres_cut'])\n",
    "y_test = multilabel_binarizer.transform(books_test['genres_cut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=True)\n",
    "tokenizer.fit_on_texts(books_train['desc_proc'])\n",
    "sequences_train = tokenizer.texts_to_sequences(books_train['desc_proc'])\n",
    "sequences_test = tokenizer.texts_to_sequences(books_test['desc_proc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(sequences_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = multilabel_binarizer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_count = []\n",
    "for genre in genres:\n",
    "    c = sum(l.count(genre) for l in list(books_train['genres_cut']))\n",
    "    genres_count.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_genres = pd.DataFrame({'genre': genres, 'count': genres_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_genres['class_weight'] = len(books_train['genres_cut']) / most_common_genres['count']\n",
    "class_weight = {}\n",
    "for i, row in most_common_genres.iterrows():\n",
    "    class_weight[i] = row['class_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/54065733/how-to-employ-the-scikit-learn-evaluation-metrics-functions-with-keras-in-python\n",
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://groups.google.com/forum/#!topic/keras-users/_sjndHbejTY\n",
    "def hn_multilabel_loss(y_true, y_pred):\n",
    "    # Avoid divide by 0\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # Multi-task loss\n",
    "    return K.mean(K.sum(- y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_to_labels(pred, threshold=0.5):\n",
    "    new_pred = []\n",
    "    for sample in pred:\n",
    "        true_pred = [0 if x < threshold else 1 for x in sample]\n",
    "        new_pred.append(true_pred)\n",
    "    return np.array(new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32668 samples, validate on 3630 samples\n",
      "Epoch 1/20\n",
      "32668/32668 [==============================] - 16s 482us/step - loss: 77.8411 - f1: 0.2291 - val_loss: 65.5623 - val_f1: 0.2110\n",
      "Epoch 2/20\n",
      "32668/32668 [==============================] - 16s 475us/step - loss: 64.9520 - f1: 0.2636 - val_loss: 61.9226 - val_f1: 0.3087\n",
      "Epoch 3/20\n",
      "32668/32668 [==============================] - 14s 423us/step - loss: 60.9857 - f1: 0.3206 - val_loss: 58.4811 - val_f1: 0.3499\n",
      "Epoch 4/20\n",
      "32668/32668 [==============================] - 14s 415us/step - loss: 58.1775 - f1: 0.3640 - val_loss: 56.4599 - val_f1: 0.3731\n",
      "Epoch 5/20\n",
      "32668/32668 [==============================] - 13s 408us/step - loss: 56.2642 - f1: 0.3957 - val_loss: 55.0317 - val_f1: 0.4003\n",
      "Epoch 6/20\n",
      "32668/32668 [==============================] - 13s 406us/step - loss: 54.9174 - f1: 0.4168 - val_loss: 54.0883 - val_f1: 0.4192\n",
      "Epoch 7/20\n",
      "32668/32668 [==============================] - 14s 423us/step - loss: 53.7947 - f1: 0.4366 - val_loss: 53.3990 - val_f1: 0.4364\n",
      "Epoch 8/20\n",
      "32668/32668 [==============================] - 14s 419us/step - loss: 52.9420 - f1: 0.4533 - val_loss: 52.7725 - val_f1: 0.4452\n",
      "Epoch 9/20\n",
      "32668/32668 [==============================] - 16s 503us/step - loss: 52.2035 - f1: 0.4656 - val_loss: 52.3025 - val_f1: 0.4536\n",
      "Epoch 10/20\n",
      "32668/32668 [==============================] - 15s 449us/step - loss: 51.5992 - f1: 0.4753 - val_loss: 51.9278 - val_f1: 0.4623\n",
      "Epoch 11/20\n",
      "32668/32668 [==============================] - 13s 409us/step - loss: 51.0093 - f1: 0.4836 - val_loss: 51.6432 - val_f1: 0.4703\n",
      "Epoch 12/20\n",
      "32668/32668 [==============================] - 16s 475us/step - loss: 50.5422 - f1: 0.4922 - val_loss: 51.3599 - val_f1: 0.4753\n",
      "Epoch 13/20\n",
      "32668/32668 [==============================] - 16s 502us/step - loss: 50.1882 - f1: 0.4989 - val_loss: 51.1203 - val_f1: 0.4723\n",
      "Epoch 14/20\n",
      "32668/32668 [==============================] - 14s 421us/step - loss: 49.7734 - f1: 0.5054 - val_loss: 50.9255 - val_f1: 0.4742\n",
      "Epoch 15/20\n",
      "32668/32668 [==============================] - 13s 397us/step - loss: 49.4625 - f1: 0.5094 - val_loss: 50.7303 - val_f1: 0.4799\n",
      "Epoch 16/20\n",
      "32668/32668 [==============================] - 12s 381us/step - loss: 49.2326 - f1: 0.5146 - val_loss: 50.7980 - val_f1: 0.4804\n",
      "Epoch 17/20\n",
      "32668/32668 [==============================] - 12s 375us/step - loss: 48.9891 - f1: 0.5174 - val_loss: 50.5905 - val_f1: 0.4781\n",
      "Epoch 18/20\n",
      "32668/32668 [==============================] - 15s 453us/step - loss: 48.7876 - f1: 0.5200 - val_loss: 50.5137 - val_f1: 0.4845\n",
      "Epoch 19/20\n",
      "32668/32668 [==============================] - 15s 456us/step - loss: 48.6204 - f1: 0.5237 - val_loss: 50.4817 - val_f1: 0.4892\n",
      "Epoch 20/20\n",
      "32668/32668 [==============================] - 15s 450us/step - loss: 48.4042 - f1: 0.5247 - val_loss: 50.4268 - val_f1: 0.4899\n"
     ]
    }
   ],
   "source": [
    "#simple feed-forward\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GlobalMaxPool1D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 20, input_length=maxlen))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(len(genres), activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss=hn_multilabel_loss, metrics=[f1])\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    EarlyStopping(patience=4),\n",
    "    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    class_weight=class_weight,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 0s 24us/step\n",
      "f1: 0.4936594356777807\n",
      "0.4948857132306783\n",
      "0.09833649643432252\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(x_test, y_test)\n",
    "print(\"{}: {}\".format(model.metrics_names[1], metrics[1]))\n",
    "y_pred = scores_to_labels(model.predict(x_test))\n",
    "print(f1_score(y_test, y_pred, average='micro'))\n",
    "print(hamming_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32668 samples, validate on 3630 samples\n",
      "Epoch 1/20\n",
      "32668/32668 [==============================] - 138s 4ms/step - loss: 63.2931 - f1: 0.3138 - val_loss: 53.2322 - val_f1: 0.4797\n",
      "Epoch 2/20\n",
      "32668/32668 [==============================] - 138s 4ms/step - loss: 49.8038 - f1: 0.5065 - val_loss: 47.2040 - val_f1: 0.5485\n",
      "Epoch 3/20\n",
      "32668/32668 [==============================] - 151s 5ms/step - loss: 44.3235 - f1: 0.5812 - val_loss: 45.7786 - val_f1: 0.5878\n",
      "Epoch 4/20\n",
      "32668/32668 [==============================] - 143s 4ms/step - loss: 40.7238 - f1: 0.6265 - val_loss: 44.4800 - val_f1: 0.5878\n",
      "Epoch 5/20\n",
      "32668/32668 [==============================] - 146s 4ms/step - loss: 37.9983 - f1: 0.6582 - val_loss: 44.3817 - val_f1: 0.5999\n",
      "Epoch 6/20\n",
      "32668/32668 [==============================] - 143s 4ms/step - loss: 35.7504 - f1: 0.6839 - val_loss: 44.4240 - val_f1: 0.6121\n",
      "Epoch 7/20\n",
      "32668/32668 [==============================] - 145s 4ms/step - loss: 33.8035 - f1: 0.7052 - val_loss: 44.7616 - val_f1: 0.6092\n",
      "Epoch 8/20\n",
      "32668/32668 [==============================] - 146s 4ms/step - loss: 32.0346 - f1: 0.7251 - val_loss: 45.5418 - val_f1: 0.6153\n",
      "Epoch 9/20\n",
      "32668/32668 [==============================] - 144s 4ms/step - loss: 30.4644 - f1: 0.7406 - val_loss: 46.1958 - val_f1: 0.6138\n"
     ]
    }
   ],
   "source": [
    "#1CNN\n",
    "from keras.layers import Activation, GlobalMaxPool1D, Dropout,Conv1D\n",
    "\n",
    "filter_length = 300\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_words, 20, input_length=maxlen))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1))\n",
    "model2.add(GlobalMaxPool1D())\n",
    "model2.add(Dense(len(genres)))\n",
    "model2.add(Activation('sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss=hn_multilabel_loss, metrics=[f1])\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(), \n",
    "    EarlyStopping(patience=4), \n",
    "    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model2.fit(x_train, y_train,\n",
    "                    class_weight=class_weight,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 5s 398us/step\n",
      "f1: 0.610508267920484\n",
      "0.6116163189037683\n",
      "0.0896523464458247\n"
     ]
    }
   ],
   "source": [
    "metrics = model2.evaluate(x_test, y_test)\n",
    "print(\"{}: {}\".format(model2.metrics_names[1], metrics[1]))\n",
    "y_pred = scores_to_labels(model2.predict(x_test))\n",
    "print(f1_score(y_test, y_pred, average='micro'))\n",
    "print(hamming_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32668 samples, validate on 3630 samples\n",
      "Epoch 1/20\n",
      "32668/32668 [==============================] - 420s 13ms/step - loss: 67.5197 - f1: 0.2334 - val_loss: 60.2534 - val_f1: 0.3006\n",
      "Epoch 2/20\n",
      "32668/32668 [==============================] - 451s 14ms/step - loss: 58.3744 - f1: 0.3729 - val_loss: 52.9559 - val_f1: 0.4405\n",
      "Epoch 3/20\n",
      "32668/32668 [==============================] - 478s 15ms/step - loss: 51.5856 - f1: 0.4752 - val_loss: 48.5368 - val_f1: 0.5188\n",
      "Epoch 4/20\n",
      "32668/32668 [==============================] - 448s 14ms/step - loss: 47.1735 - f1: 0.5376 - val_loss: 46.1364 - val_f1: 0.5436\n",
      "Epoch 5/20\n",
      "32668/32668 [==============================] - 402s 12ms/step - loss: 44.4524 - f1: 0.5745 - val_loss: 45.9352 - val_f1: 0.5673\n",
      "Epoch 6/20\n",
      "32668/32668 [==============================] - 801s 25ms/step - loss: 42.5503 - f1: 0.6017 - val_loss: 44.5109 - val_f1: 0.5915\n",
      "Epoch 7/20\n",
      "32668/32668 [==============================] - 838s 26ms/step - loss: 40.9299 - f1: 0.6207 - val_loss: 43.5926 - val_f1: 0.5922\n",
      "Epoch 8/20\n",
      "32668/32668 [==============================] - 1163s 36ms/step - loss: 39.3535 - f1: 0.6412 - val_loss: 43.3582 - val_f1: 0.5968\n",
      "Epoch 9/20\n",
      "32668/32668 [==============================] - 714s 22ms/step - loss: 38.1329 - f1: 0.6572 - val_loss: 43.3847 - val_f1: 0.6103\n",
      "Epoch 10/20\n",
      "32668/32668 [==============================] - 1145s 35ms/step - loss: 36.9662 - f1: 0.6693 - val_loss: 42.3994 - val_f1: 0.6164\n",
      "Epoch 11/20\n",
      "32668/32668 [==============================] - 578s 18ms/step - loss: 35.9407 - f1: 0.6828 - val_loss: 42.4464 - val_f1: 0.6264\n",
      "Epoch 12/20\n",
      "32668/32668 [==============================] - 891s 27ms/step - loss: 35.0628 - f1: 0.6929 - val_loss: 42.6185 - val_f1: 0.6292\n",
      "Epoch 13/20\n",
      "32668/32668 [==============================] - 549s 17ms/step - loss: 34.1088 - f1: 0.7022 - val_loss: 42.6677 - val_f1: 0.6325\n",
      "Epoch 14/20\n",
      "32668/32668 [==============================] - 888s 27ms/step - loss: 33.4398 - f1: 0.7113 - val_loss: 42.6556 - val_f1: 0.6361\n"
     ]
    }
   ],
   "source": [
    "#1LSTM\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words, 20, input_length=maxlen))\n",
    "model3.add(Dropout(0.15))\n",
    "model3.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model3.add(Dense(len(genres)))\n",
    "model3.add(Activation('sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='adam', loss=hn_multilabel_loss, metrics=[f1])\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(), \n",
    "    EarlyStopping(patience=4), \n",
    "    ModelCheckpoint(filepath='model-lstm.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model3.fit(x_train, y_train,\n",
    "                    class_weight=class_weight,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(x, y, model):\n",
    "    best_f1=0\n",
    "    best_t = 0\n",
    "    for t in np.linspace(0.0, 0.8, 25):\n",
    "        f1 = f1_score(y, scores_to_labels(model3.predict(x), t), average='micro')\n",
    "        if f1 > best_f1:\n",
    "            best_t=t\n",
    "            best_f1=f1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 50s 4ms/step\n",
      "f1: 0.6333734587071433\n",
      "0.6455325735190657\n",
      "0.09055095468138946\n"
     ]
    }
   ],
   "source": [
    "metrics = model3.evaluate(x_test, y_test)\n",
    "print(\"{}: {}\".format(model3.metrics_names[1], metrics[1]))\n",
    "#threshold = optimize_threshold(x_val, y_val, model3)\n",
    "y_pred = scores_to_labels(model3.predict(x_test), 0.35)\n",
    "print(f1_score(y_test, y_pred, average='micro'))\n",
    "print(hamming_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32668 samples, validate on 3630 samples\n",
      "Epoch 1/20\n",
      "32668/32668 [==============================] - 136s 4ms/step - loss: 61.3964 - f1: 0.3342 - val_loss: 55.9522 - val_f1: 0.4045\n",
      "Epoch 2/20\n",
      "32668/32668 [==============================] - 80s 2ms/step - loss: 55.1468 - f1: 0.4319 - val_loss: 52.0515 - val_f1: 0.4553\n",
      "Epoch 3/20\n",
      "32668/32668 [==============================] - 78s 2ms/step - loss: 49.9332 - f1: 0.5065 - val_loss: 48.7640 - val_f1: 0.5000\n",
      "Epoch 4/20\n",
      "32668/32668 [==============================] - 78s 2ms/step - loss: 45.4665 - f1: 0.5669 - val_loss: 46.3066 - val_f1: 0.5519\n",
      "Epoch 5/20\n",
      "32668/32668 [==============================] - 78s 2ms/step - loss: 42.0320 - f1: 0.6120 - val_loss: 45.7539 - val_f1: 0.5758\n",
      "Epoch 6/20\n",
      "32668/32668 [==============================] - 78s 2ms/step - loss: 39.5189 - f1: 0.6436 - val_loss: 46.5010 - val_f1: 0.5723\n",
      "Epoch 7/20\n",
      "32668/32668 [==============================] - 78s 2ms/step - loss: 37.5537 - f1: 0.6667 - val_loss: 45.7366 - val_f1: 0.5962\n",
      "Epoch 8/20\n",
      "32668/32668 [==============================] - 97s 3ms/step - loss: 35.9445 - f1: 0.6853 - val_loss: 46.2653 - val_f1: 0.5942\n",
      "Epoch 9/20\n",
      "32668/32668 [==============================] - 134s 4ms/step - loss: 34.5692 - f1: 0.6999 - val_loss: 46.4743 - val_f1: 0.5976\n",
      "Epoch 10/20\n",
      "32668/32668 [==============================] - 132s 4ms/step - loss: 33.3540 - f1: 0.7124 - val_loss: 46.8029 - val_f1: 0.6005\n",
      "Epoch 11/20\n",
      "32668/32668 [==============================] - 133s 4ms/step - loss: 32.2784 - f1: 0.7234 - val_loss: 47.8222 - val_f1: 0.6092\n",
      "Epoch 12/20\n",
      "32668/32668 [==============================] - 133s 4ms/step - loss: 31.3788 - f1: 0.7326 - val_loss: 47.7970 - val_f1: 0.6083\n",
      "Epoch 13/20\n",
      "32668/32668 [==============================] - 133s 4ms/step - loss: 30.5682 - f1: 0.7413 - val_loss: 48.8240 - val_f1: 0.6085\n",
      "Epoch 14/20\n",
      "32668/32668 [==============================] - 133s 4ms/step - loss: 29.8682 - f1: 0.7470 - val_loss: 48.7379 - val_f1: 0.6067\n"
     ]
    }
   ],
   "source": [
    "#2CNN + 1LSTM\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(max_words, 20, input_length=maxlen))\n",
    "model4.add(Dropout(0.1))\n",
    "model4.add(Conv1D(128, 5))\n",
    "model4.add(MaxPooling1D(5))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(Conv1D(128, 5))\n",
    "model4.add(MaxPooling1D(5))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model4.add(Dense(len(genres)))\n",
    "model4.add(Activation('sigmoid'))\n",
    "\n",
    "model4.compile(optimizer='adam', loss=hn_multilabel_loss, metrics=[f1])\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(), \n",
    "    EarlyStopping(patience=7), \n",
    "    ModelCheckpoint(filepath='model-lstm-cnn.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model4.fit(x_train, y_train,\n",
    "                    class_weight=class_weight,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 11s 934us/step\n",
      "f1: 0.6029014637072881\n",
      "0.6152377968008887\n",
      "0.09709641706924316\n"
     ]
    }
   ],
   "source": [
    "metrics = model4.evaluate(x_test, y_test)\n",
    "print(\"{}: {}\".format(model4.metrics_names[1], metrics[1]))\n",
    "y_pred = scores_to_labels(model4.predict(x_test), 0.35)\n",
    "print(f1_score(y_test, y_pred, average='micro'))\n",
    "print(hamming_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32668 samples, validate on 3630 samples\n",
      "Epoch 1/20\n",
      "32668/32668 [==============================] - 278s 9ms/step - loss: 62.9918 - f1: 0.3006 - val_loss: 55.3203 - val_f1: 0.4052\n",
      "Epoch 2/20\n",
      "32668/32668 [==============================] - 227s 7ms/step - loss: 51.8872 - f1: 0.4744 - val_loss: 47.2303 - val_f1: 0.5291\n",
      "Epoch 3/20\n",
      "32668/32668 [==============================] - 147s 4ms/step - loss: 45.0039 - f1: 0.5678 - val_loss: 45.1097 - val_f1: 0.5662\n",
      "Epoch 4/20\n",
      "32668/32668 [==============================] - 161s 5ms/step - loss: 41.5756 - f1: 0.6125 - val_loss: 44.1323 - val_f1: 0.5867\n",
      "Epoch 5/20\n",
      "32668/32668 [==============================] - 206s 6ms/step - loss: 38.8836 - f1: 0.6494 - val_loss: 43.8120 - val_f1: 0.5966\n",
      "Epoch 6/20\n",
      "32668/32668 [==============================] - 286s 9ms/step - loss: 36.9408 - f1: 0.6709 - val_loss: 43.9086 - val_f1: 0.6061\n",
      "Epoch 7/20\n",
      "32668/32668 [==============================] - 287s 9ms/step - loss: 35.3102 - f1: 0.6911 - val_loss: 44.2228 - val_f1: 0.6104\n",
      "Epoch 8/20\n",
      "32668/32668 [==============================] - 289s 9ms/step - loss: 34.0597 - f1: 0.7048 - val_loss: 44.7923 - val_f1: 0.6157\n",
      "Epoch 9/20\n",
      "32668/32668 [==============================] - 295s 9ms/step - loss: 32.8637 - f1: 0.7169 - val_loss: 45.0309 - val_f1: 0.6172\n",
      "Epoch 10/20\n",
      "32668/32668 [==============================] - 170s 5ms/step - loss: 31.6783 - f1: 0.7299 - val_loss: 46.3423 - val_f1: 0.6218\n"
     ]
    }
   ],
   "source": [
    "#1CNN + 1LSTM\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Embedding(max_words, 20, input_length=maxlen))\n",
    "model5.add(Dropout(0.1))\n",
    "model5.add(Conv1D(128, 5))\n",
    "model5.add(MaxPooling1D(5))\n",
    "model5.add(Dropout(0.2))\n",
    "model5.add(LSTM(128, dropout=0.2, recurrent_dropout=0.3))\n",
    "model5.add(Dense(len(genres)))\n",
    "model5.add(Activation('sigmoid'))\n",
    "\n",
    "model5.compile(optimizer='adam', loss=hn_multilabel_loss, metrics=[f1])\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(), \n",
    "    EarlyStopping(patience=5), \n",
    "    ModelCheckpoint(filepath='model-lstm-cnn2.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model5.fit(x_train, y_train,\n",
    "                    class_weight=class_weight,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 13s 1ms/step\n",
      "f1: 0.6117301418510064\n",
      "0.6233747999213726\n",
      "0.0964170692431562\n"
     ]
    }
   ],
   "source": [
    "metrics = model5.evaluate(x_test, y_test)\n",
    "print(\"{}: {}\".format(model5.metrics_names[1], metrics[1]))\n",
    "y_pred = scores_to_labels(model5.predict(x_test), 0.35)\n",
    "print(f1_score(y_test, y_pred, average='micro'))\n",
    "print(hamming_loss(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
